{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Mile of Miles: Building a Monte Carlo statistical model in 3 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running clubs in the Greater Southampton area annually compete in the 'Mile of Miles' team competition, typically held in June. Given the cancellation of events in 2020, my club decided to replicate it with our own, virtual event. In this virtual Mile of Miles, there are 12 teams of 10 runners, drawn randomly from a hat beforehand. Each team member completes the fastest 1 mile time they can, at any time, on any net zero elevation route in a set week.  The team's time is the summation of the each team member's 1 mile time. Every second counts!\n",
    "\n",
    "Before the race was completed, I was asked if I fancied guessing the outcome. Given this little challenge, I decided to build a simple statistical model to simulate the event using some simple, data-driven statistics. I did this in 3 days in June, 2020, in between doing my PhD in Astrophysics at University of Southampton. In this notebook I step through the method I employ here, outlining both its strengths and pitfalls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 mile races are uncommon. However, in addition to the annual Mile of Mile events, there is an annual summer series of 1 mile events, known as the 'Magic Mile', hosted on Southampton Common. We can therefore use historical data from these events as input data to our model.\n",
    "\n",
    "In order to estimate each individual's 1 mile time, we need to find a relation between some readily available data and the limited historic 1 mile available to us. Data from timed 5k events are readily accessible, in the form of parkrun results – parkruns are free, weekly, timed 5k events hosted across UK (and worldwide). Southampton parkrun regularly attracts over 1000 runners, and most runners in this competition have completed at least one of these events in 2020, so it sets a standard we can measure from.\n",
    "\n",
    "I pulled the fastest 5km time of 2020 for each athlete in the competition, run at Southampton parkrun. For the small number of athletes that this didn't apply to, I took the individual's best at another parkrun in 2020, or their best 5km time in 2019, or in the worst case, a general estimation from their training group, in that order. \n",
    "\n",
    "I then pulled the last 3 years’ worth of 1 mile race data (from both Mile of Miles and Magic Mile competitions) and matched results to any participant running in this virtual competition. Many runners have not completed a 1 mile race, while some have completed many. Both mile and 5k data for this analysis were stored on a spreadsheet (saved as a .csv file), which is where this analysis begins.\n",
    "\n",
    "It is these data that we will use to form a predictive relation, which forms the basis of the Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Mile_of_miles.csv' does not exist: b'Mile_of_miles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f8d6f957a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the csv file containing runners and best 2020 5k & (all-time) mile times:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mile_of_miles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set path to .csv file appropriately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Do some data cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Mile_of_miles.csv' does not exist: b'Mile_of_miles.csv'"
     ]
    }
   ],
   "source": [
    "# Read the csv file containing runners and best 2020 5k & (all-time) mile times:\n",
    "\n",
    "df = pd.read_csv(\"Mile_of_miles.csv\") # Set path to .csv file appropriately \n",
    "\n",
    "# Do some data cleaning\n",
    "\n",
    "df = df.drop(columns=['Team 1']) # Drop unneccesary columns\n",
    "\n",
    "df = df.dropna(how='all')  # Drop rows if all NaNs (i.e. between groups on the spreadsheet or added rows)\n",
    "\n",
    "df.head() # The resulting dataframe looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index represents an individual athlete (names removed for confidentiality). The '5k' column shows their selected time (i.e. best 5k of 2020, 2019...etc, as above). All other columns show the mile time from past 'Mile of Mile' (MoM) events or 'Magic Mile' (MM) events -- NaN values if the athlete did not compete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data search for 5k/Mile times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find and match any mile results with 5k performance. The limitation here is that mile races were across different years and we're comparing to only a single 5k time (in 2020). We need to keep this limitation in mind.\n",
    "\n",
    "Let's first convert race times into seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mins_secs(time):\n",
    "    \"Function to convert minutes (type: strings) into seconds (type: floats)\"\n",
    "    \n",
    "    if len(time) > 6: # If format is XX:XX:XX \n",
    "        seconds = float(time[:2])*60 + float(time[3:5]) # Convert times to seconds\n",
    "        \n",
    "    elif len(str(time)) < 6: # If format is XX:XX\n",
    "        seconds = float(time[:2])*60 + float(time[3:])  # Convert times to seconds\n",
    "    \n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 5k times to seconds and insert new column\n",
    "\n",
    "col_names = df.columns  # Obtain column names as list\n",
    "\n",
    "new_col_names = col_names.insert(1,'5k_secs')  # Choose position for new columnname \n",
    "\n",
    "df['5k_secs'] = [mins_secs(time) for time in df['5k'].values]  # Populate new column as 5k time in seconds\n",
    "\n",
    "df = df[new_col_names]  # Re-order daatframe appropriately\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's populate a sample of mile times, set against the corresponding 5k time, over all athletes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe consisting of only mile times:\n",
    "\n",
    "df_miles = df.drop(columns=['5k', '5k_secs']) \n",
    "\n",
    "df_miles = df_miles.dropna(how='all')  # Drop any rows where an athlete has no historic mile results\n",
    "\n",
    "miletimes = []  # Initialise our list of historic mile times\n",
    "\n",
    "for i in range(len(df_miles)):\n",
    "    mask = pd.isna(df_miles.iloc[i].values)  # For each row create a mask of NaN values\n",
    "    miletimes.append(df_miles.iloc[i][~mask].values.tolist())  # Then append a list of all non-Nan values into a list\n",
    "\n",
    "miletimes = [mins_secs(time) for sublist in miletimes for time in sublist]  # Flatten into a single 1d list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get our corresponding list of 5k times:\n",
    "\n",
    "indexes = df_miles.index  # Find which athletes have historic mile time \n",
    "\n",
    "df_filtered = df[df.index.isin(indexes)]  # Create a filtered dataframe of only these athletes\n",
    "\n",
    "mile_count = (len(df_miles.columns) - pd.isna(df_miles).sum(axis=1)).values  # Count how many mile races they've done\n",
    "\n",
    "fivektimes = []  # Initialise our list of historic 5k times\n",
    "\n",
    "for i,val in enumerate(mile_count):  # For each athlete in the filtered dataframe\n",
    "    for j in range(val):  \n",
    "        fivektimes.append(df_filtered['5k_secs'].values[i])  # Append the athlete's 5k time for as many mile races \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of all the historic mile times in our sample, matched with the 5k times that each athlete recorded in 2020, as a measure of overall ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When investigating the relationship between historic mile and 5k performance we find a linear relationship -- faster 5k runners run faster 1 mile races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the underlying relationship:\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('Mile time (s)')\n",
    "plt.ylabel('5k time (s)')\n",
    "\n",
    "plt.scatter(miletimes, fivektimes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this relationship is approximately linear, we want to describe this relation with a linear model. There exist numerous methods to achieve this, including manually defining a loss function using maximum likelihood statistics. Due to the simple nature of these data however, a standard least-squares methodology (i.e. the scipy.optimize function curve_fit) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_line(x, m, c):\n",
    "    ''' Function to define our linear model '''\n",
    "    y = m*x + c\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform our linear regression using curve_fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(linear_line, miletimes, fivektimes)  # curve_fit minimisation routine\n",
    "m_min, c_min = popt  # Initial estimate model parameters\n",
    "\n",
    "x_plot = np.arange(275,700)  # Range of mile times in seconds\n",
    "\n",
    "Model = linear_line(x_plot, m_min, c_min)  # Return our best fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise this:\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('Mile time (s)')\n",
    "plt.ylabel('5k time (s)')\n",
    "\n",
    "plt.scatter(miletimes, fivektimes)\n",
    "\n",
    "plt.plot(x_plot, Model, color='orange')  # Plot on our model\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap-with-replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis we will employ simple, frequentist statistics. In order to describe the uncertainty on this linear model we will employ a bootstrap-with-replacement method, using the function below. \n",
    "\n",
    "This method randomly selects data points from the sample, up to a number equal to the total number of data in the sample, and is allowed to re-select the same data points multiple times. For N iterations of this process, we fit this new ‘bootstrapped’ dataset with our linear model and form a discrete distribution of model parameters from all iterations.\n",
    "\n",
    "It is worth noting here that the data appear to suffer from some amount of observational bias towards faster runners – particularly those with 5k times < ~28mins (or 1680 seconds). The median parkrun time of runners in my club is found to be ~26 minutes (see modelling parkrun results notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bootstrap_Continuum(N, Xs, Ys, m_guess, c_guess):\n",
    "    ''' Function to perform a bootstrapping-with-replacement routine and \n",
    "    return the resulting distribution of parameters '''\n",
    "    \n",
    "    c_list = [] \n",
    "    m_list = []\n",
    "\n",
    "    startindex = list(range(len(Xs)))  # Set up index range: (0 - last index in data set)\n",
    "        \n",
    "    for iter in range(N):\n",
    "       \n",
    "        index = np.random.choice(startindex, len(startindex), replace=True)                \n",
    "        Rnd_Xs = Xs[index]  # Draws selection of random data from data-set \n",
    "        Rnd_Ys = Ys[index]  \n",
    "\n",
    "        Boot_opt, Boot_cov = curve_fit(linear_line, Rnd_Xs, Rnd_Ys)  # Fit bootstrapped dataset as before\n",
    "        m_boot, c_boot = Boot_opt  \n",
    "   \n",
    "        m_list.append(m_boot)  # Add these parameter fits to the list\n",
    "        c_list.append(c_boot) \n",
    "        \n",
    "\n",
    "    c_list = np.array(c_list)  # Return arrays of parameters\n",
    "    m_list = np.array(m_list)\n",
    "\n",
    "    return m_list, c_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the bootstrap over N iterations:\n",
    "\n",
    "N = 2000  # Number of bootstrap iterations\n",
    "m_list, c_list = Bootstrap_Continuum(N, np.array(miletimes), np.array(fivektimes), m_min, c_min)  # Return parameter lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now plot the N bootstrapped models, which directly encapsulates uncertainty in the model\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('Mile time (s)')\n",
    "plt.ylabel('5k time (s)')\n",
    "\n",
    "plt.scatter(miletimes, fivektimes, s=4)\n",
    "\n",
    "for i in range(N):  # Plot each bootstrap iteration\n",
    "    plt.plot(x_plot, linear_line(x_plot, m_list[i], c_list[i]), color='orange', alpha=0.1)  \n",
    "\n",
    "plt.plot(x_plot, Model, color='black')  # And plot the maximum likelihood model\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap inherently creates confidence limits on our model, which we can utilise in this analysis. Here, I make the assumption that these errors are Gaussian distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate all the bootstrap model outputs together\n",
    "bootstrap_lines = [linear_line(x_plot, m_list[i], c_list[i]) for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the ideal bootstrap fit from the median, and 16/84 percentiles used for 1 sigma errors\n",
    "low_sig, bootstrap_fit, upp_sig = np.percentile(bootstrap_lines, [16, 50, 84], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these errors to be Gaussian, we can calculate the standard deviation across all possible mile times \n",
    "std_devs = (upp_sig - low_sig)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 1 sigma confidence intervals which constrain our model\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('Mile time (s)')\n",
    "plt.ylabel('5k time (s)')\n",
    "\n",
    "plt.scatter(miletimes, fivektimes)\n",
    "\n",
    "plt.fill_between(x_plot, low_sig, upp_sig, color='orange', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These errors appear relatively small (but are confirmed seperately by an MCMC method) and suggest the linear model is well constrained to these sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Monte Carlo method can now be implemented to simulate N iterations of fake data, to effectively sample from our model. Using the 1-sigma errors constrained from the bootstrap, we can use the assumption that these errors are Gaussian distributed around the maximum-likelihood fit.\n",
    "\n",
    "Here we define a Monte Carlo function to run over N_sim iterations, before running this function to effectively simulate the race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC(N_sim, Model_params, std_devs, x_plot):\n",
    "    \"\"\"Monte Carlo function run over N_sims, which returns a dataframe containing results for \n",
    "    all 12 teams (in columns), over every iteration (as rows)\"\"\"\n",
    "    \n",
    "    df_MC = pd.DataFrame()  # Initialise results dataframe\n",
    "        \n",
    "    All_Times = []    \n",
    "        \n",
    "    Team_numbers = np.arange(1,13)  \n",
    "    \n",
    "    m_min, c_min = Model_params\n",
    "        \n",
    "    model_predictions = df['5k_secs'].apply(lambda time: (time - c_min) / m_min)  # 'Exact' predicted times from model\n",
    "    \n",
    "    for n in range(N_sim):  # Start MC running over N_sim iterations\n",
    "            \n",
    "        # Simulate mile times assigning Gaussian errors from the pre-defined model\n",
    "        mile_draws = [np.random.normal(time, std_devs[np.argwhere(x_plot == int(time))[0][0]])  \n",
    "                          for time in model_predictions]\n",
    "        \n",
    "        Team_times = []\n",
    "        for team in Team_numbers:  # Loop over each team\n",
    "            \n",
    "            Times = mile_draws[(team-1)*10:team*10]  # Slice out each team's (of 10) times\n",
    "            Team_times.append(np.sum(Times))  # Collect the team's total time (i.e. the sum of times)\n",
    "            \n",
    "        All_Times.append(Team_times)  # Append to a master list (this is faster than appending straight to a dataframe)\n",
    "\n",
    "    # The MC is complete, so now we collect the output into the results dataframe, one team at a time:\n",
    "    for i in range(len(Team_numbers)):\n",
    "        Team_N_times = []\n",
    "        for sublist in All_Times:\n",
    "            Team_N_times.append(sublist[i])  # Collect all times for Team 'N' (where here, 1 <= N <= 12)\n",
    "    \n",
    "        df_MC['Team_{}'.format(i+1)] = Team_N_times  # Input this list as a new column to fill our results dataframe\n",
    "                    \n",
    "    return df_MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of MC iterations and run the MC with the necessary inputs\n",
    "N_sim = 10000\n",
    "df_MC = MC(N_sim, [m_min, c_min], std_devs, x_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results consist of 10,000 simulated outcomes from the Monte-Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output dataframe takes this form (no. rows = N_sim & times displayed in seconds):\n",
    "df_MC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're interested not in the team's time, but their position, so we determine their finishing positions through:\n",
    "df_rank = df_MC.rank(axis=1)\n",
    "\n",
    "# Which returns the output dataframe and the finishing position of each team with each MC iteration:\n",
    "df_rank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a df_Results dataframe which gives the frequency (in % form) that each team finished in a \n",
    "# given position over all MC iterations\n",
    "\n",
    "Indexes = ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th', '11th', '12th']  # Positions\n",
    "\n",
    "df_Results = pd.DataFrame(columns = df_rank.columns, index=Indexes)  # Initialise dataframe\n",
    "\n",
    "for i in df_rank:  # For each team\n",
    "    Series = df_rank['{}'.format(i)].value_counts()  # Return value counts of position\n",
    "    \n",
    "    Frequency = np.zeros(12)  # Initialise frequency array\n",
    "    \n",
    "    for j,vals in enumerate(Series.index):\n",
    "        Index = int(Series.index[j]) - 1\n",
    "        Frequency[Index] = 100*Series.values[j]/N_sim  # Calculate % values of positions\n",
    "    \n",
    "    df_Results['{}'.format(i)] = Frequency  # Fill in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which returns the full dataframe of % finishes in each position:\n",
    "df_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the results from the Monte-Carlo in a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly with some dataframe styling, setting a colour scale which corresponds to position %:\n",
    "\n",
    "with pd.option_context('display.precision', 4):\n",
    "\n",
    "    cm = sns.light_palette(\"orange\", as_cmap=True)\n",
    "\n",
    "    Display = df_Results.style.background_gradient(cmap=cm)\n",
    "    \n",
    "Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can collect our results of the MC and reorder them to display the frequency \n",
    "# of finishing position both graphically and within a pandas DataFrame.\n",
    "\n",
    "dic = {}  # Empty dictionary\n",
    "\n",
    "for i,val in enumerate(df_Results.idxmax().to_list()):  # Identify index of maximum\n",
    "    b = re.split('(\\d+)', val)  # Split numbers and letters\n",
    "    dic['{}'.format(b[1])] = 'Team_{}'.format(i+1)  # Fill dictionary with \n",
    "    \n",
    "Order = []\n",
    "for i in range(len(df_Results)):\n",
    "    Order.append(dic['{}'.format(i+1)])  # Set order based on team position\n",
    "\n",
    "df_Ordered = df_Results[Order]  # Create a new dataframe ordered by this logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's implement the same styling as before on the new dataframe:\n",
    "\n",
    "with pd.option_context('display.precision', 4):\n",
    "\n",
    "    cm = sns.light_palette(\"orange\", as_cmap=True)\n",
    "\n",
    "    Display_ordered = df_Ordered.style.background_gradient(cmap=cm)\n",
    "    \n",
    "# It is now a lot clearer!\n",
    "Display_ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a grid of subplots to show these predicted results\n",
    "\n",
    "fig, ax = plt.subplots(4,3, figsize=(22,12))\n",
    "fig.subplots_adjust(wspace = 0, hspace = 0)\n",
    "\n",
    "axes = [ax[0,0], ax[0,1], ax[0,2], ax[1,0], ax[1,1], ax[1,2], ax[2,0], ax[2,1], ax[2,2], ax[3,0], ax[3,1], ax[3,2]]\n",
    "\n",
    "for i,x in enumerate(axes):\n",
    "    x.set_ylim(0,110)\n",
    "    \n",
    "    if i > 8:  # Set x labels at bottom of grid only\n",
    "        x.set_xlabel('Position')\n",
    "        x.set_xticks(np.arange(14))\n",
    "        \n",
    "        xlabels = np.array([''])\n",
    "        xlabels = np.append(xlabels, np.arange(1,13))\n",
    "        xlabels = np.append(xlabels, '')\n",
    "        x.set_xticklabels(xlabels)         \n",
    "    else:\n",
    "        x.set_xticklabels([])\n",
    "        \n",
    "    if i%3 == 0:  # Set y labels at LHS of grid only\n",
    "        x.set_ylabel('% Frequency')\n",
    "        x.set_yticks([25,50,75,100])\n",
    "    else:\n",
    "        x.set_yticklabels([])\n",
    "        \n",
    "    if i == 3 or i == 8:  # Add Team numbers\n",
    "        x.text(0.95, 0.85,'Team {}'.format(i+1), fontsize=16, fontweight='bold', ha='right', transform=x.transAxes)\n",
    "    else:\n",
    "        x.text(0.05, 0.85,'Team {}'.format(i+1), fontsize=16, fontweight='bold', ha='left', transform=x.transAxes)\n",
    "    \n",
    "    x.bar(np.arange(1,13), df_Results['Team_{}'.format(i+1)].values, color='orange')  # Plot bar charts\n",
    "    \n",
    "    for j,val in enumerate(df_Results['Team_{}'.format(i+1)].values):\n",
    "        if val == 0.0:\n",
    "            continue\n",
    "        x.text(np.arange(1,13)[j], val+3, val, fontsize=10, ha='center')  # Add %values to bars\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediciton Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC has created a distribution of possible outcomes, with some notable outputs. The model appears very confident in choosing a winner in Team 9: ~93% and last place: Team 1: >99%! The greatest uncertainty in finishing position appears in predominantly in three clusters: from 2nd - 6th, 7th - 8th and 9th - 11th, with little overlap in between. The model therefore appears to be broadly confident in grouping teams into five distinct categories: first place, clusters 1-3 and last place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Race Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The race results were published in the following week of June, which can now be taken for post-prediction analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the new results data file\n",
    "\n",
    "df_Race = pd.read_csv(\"Mile_of_miles_full_results.csv\")  # Set path to .csv file appropriately \n",
    "\n",
    "df_Race = df_Race.drop(columns=['Team 1']) # Drop unneccesary column names as before\n",
    "\n",
    "df_Race = df_Race.dropna(how='all')  # Drop rows if all NaNs (i.e. between groups on the spreadsheet or added rows)\n",
    "\n",
    "\n",
    "# Convert both 5k and race result mile times (i.e. 'Actual' column) to seconds and insert these new columns \n",
    "\n",
    "col_names = df_Race.columns  # Obtain column names as list\n",
    "\n",
    "new_col_names = col_names.insert(1, '5k_secs')  # Choose position for new column name \n",
    "new_col_names = new_col_names.insert(2, 'Actual_secs')  # Choose position for second new column name \n",
    "\n",
    "df_Race['5k_secs'] = [mins_secs(time) for time in df_Race['5k'].values]  # Populate new column as 5k time in seconds\n",
    "df_Race['Actual_secs'] = [mins_secs(time) for time in df_Race['Actual'].values]  # Populate new column as actual mile time in seconds\n",
    "\n",
    "df_Race = df_Race[new_col_names]  # Re-order daatframe appropriately\n",
    "\n",
    "df_Race.head() # The resulting dataframe looks like this ('Actual_secs' is the results column of interest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Team_times = []  # Initialise list\n",
    "\n",
    "mile_times = df_Race['Actual_secs'].values  # Extract race results\n",
    "\n",
    "for i in range(12):  # Loop over 12 teams:\n",
    "    team_total_time = np.sum(mile_times[i*10:10*(i+1)])  # We can sum over the results in blocks of 10 (= team size)\n",
    "    Team_times.append(team_total_time)\n",
    "\n",
    "print(Team_times)  # Returns a list of results, in order Team 1 - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use another dictionary to order this list properly\n",
    "\n",
    "d = {}  # Initialise dictionary\n",
    "Team_order = []\n",
    "\n",
    "Sorted_times = np.sort(Team_times)  # Sort by race time\n",
    "\n",
    "for i,val in enumerate(df_Results):    \n",
    "    d['{}'.format(Team_times[i])] = '{}'.format(val) # Fill the dictionary of times matched to teams\n",
    "\n",
    "for time in Sorted_times:\n",
    "    Team_order.append(d['{}'.format(time)])  # Create the ordered list by selecting times one-by-one\n",
    "\n",
    "    \n",
    "# Let's save this output to a dictionary, whilst printing these out to see which team finishes where:    \n",
    "dict_pos = {}\n",
    "    \n",
    "for i,val in enumerate(Team_order):\n",
    "    dict_pos['{}'.format(val)] = str(i+1)\n",
    "    print('Pos {0}: {1}'.format(i+1, val))  # From 1st place to 12th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we know Team 5 was the eventual winner! Let's visualise this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's visualise these results in our prediction dataframe (final position highlighted in green)\n",
    "\n",
    "def style_specific_cell(x):\n",
    "    with pd.option_context('display.precision', 2):\n",
    "        colour = 'background-color: lightgreen'\n",
    "        df1 = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "        for i,val in enumerate(Team_order):\n",
    "            loc = np.argwhere(val == x.columns.values)[0][0]\n",
    "            df1.iloc[i,loc] = colour  # Highlight each finishing position in green\n",
    "\n",
    "        return df1\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "df_Ordered.style.apply(style_specific_cell, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's overlay these final positions onto our grid of subplots similarly\n",
    "\n",
    "fig, ax = plt.subplots(4,3, figsize=(22,12))\n",
    "fig.subplots_adjust(wspace = 0, hspace = 0)\n",
    "\n",
    "axes = [ax[0,0], ax[0,1], ax[0,2], ax[1,0], ax[1,1], ax[1,2], ax[2,0], ax[2,1], ax[2,2], ax[3,0], ax[3,1], ax[3,2]]\n",
    "\n",
    "for i,x in enumerate(axes):\n",
    "    x.set_ylim(0,110)\n",
    "    \n",
    "    if i > 8:  # Set x labels at bottom of grid only\n",
    "        x.set_xlabel('Position')\n",
    "        x.set_xticks(np.arange(14))\n",
    "        \n",
    "        xlabels = np.array([''])\n",
    "        xlabels = np.append(xlabels, np.arange(1,13))\n",
    "        xlabels = np.append(xlabels, '')\n",
    "        x.set_xticklabels(xlabels)         \n",
    "    else:\n",
    "        x.set_xticklabels([])\n",
    "        \n",
    "    if i%3 == 0:  # Set y labels at LHS of grid only\n",
    "        x.set_ylabel('% Frequency')\n",
    "        x.set_yticks([25,50,75,100])\n",
    "    else:\n",
    "        x.set_yticklabels([])\n",
    "        \n",
    "    if i == 3 or i == 8:  # Add Team numbers\n",
    "        x.text(0.95, 0.85,'Team {}'.format(i+1), fontsize=16, fontweight='bold', ha='right', transform=x.transAxes)\n",
    "    else:\n",
    "        x.text(0.05, 0.85,'Team {}'.format(i+1), fontsize=16, fontweight='bold', ha='left', transform=x.transAxes)\n",
    "    \n",
    "    x.bar(np.arange(1,13), df_Results['Team_{}'.format(i+1)].values, color='orange')  # Plot bar charts\n",
    "    \n",
    "    # Now, plot actual finishing position in green\n",
    "    Bars = np.zeros(12) \n",
    "    Position = int(dict_pos['Team_{}'.format(i+1)]) - 1  # Identify team position from dictionary\n",
    "    Bars[Position] = df_Results['Team_{}'.format(i+1)].values[Position]\n",
    "    \n",
    "    x.bar(np.arange(1,13), Bars, color='lightgreen')  # Plot bar chart overlay\n",
    "    \n",
    "    \n",
    "    for j,val in enumerate(df_Results['Team_{}'.format(i+1)].values):\n",
    "        if val == 0.0:\n",
    "            continue\n",
    "        x.text(np.arange(1,13)[j], val+3, val, fontsize=10, ha='center')  # Add %values to bars\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted vs Actual comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's round off this analysis with a direct comparison between predicted time from our model to the race results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model predictions and create a dataframe of actual/predicted mile times (in secs) \n",
    "\n",
    "model_predictions = df['5k_secs'].apply(lambda time: (time - c_min) / m_min)\n",
    "\n",
    "df_comp = pd.DataFrame({'Actual':df_Race['Actual_secs'], 'Predicted':model_predictions})\n",
    "\n",
    "# Our new dataframe:\n",
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's take the difference between race times and predicted times\n",
    "\n",
    "diffs = df_comp['Actual'] - df_comp['Predicted']  # Time difference\n",
    "\n",
    "\n",
    "# And create a histogram of these results:\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.xlabel('Actual Time - Prediction Time (s)', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "\n",
    "plt.hist(diffs, edgecolor='#E6E6E6', color='#EE6666', bins=30)  # Plot histogram\n",
    "\n",
    "plt.axvline(np.median(diffs), color='black', linestyle='--')  # Plot median time difference\n",
    "\n",
    "plt.xlim(-150,150)\n",
    "\n",
    "plt.text(47,17, 'median = +{}s'.format(int(np.median(diffs))), fontsize=16)  # Display median time difference\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The precentage of mile times within +-11 seconds of their prediction:\n",
    "print('Predictions within +- median: ', round(100*np.sum(abs(diffs) < 11) / len(diffs), 1),'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the model predicted mile time is a (median) average of 11 seconds less than the actual mile time of runners in this sample. On average therefore, it tends towards predicting 'optimistic' (i.e. faster) mile times. Exactly a third of our predictions fall within 11 seconds of race results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: where the model falls down and how to improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 12 teams in the model, 6 produced results which were considered extremely low frequency occurrences (<1% in each case), including that of the winning team (Team 5). More broadly however, we find that the median prediction time is (only) 11 seconds out and the general trend of position predictions is certainly better than random chance, broadly correlating with our model predictions, but with large variance on these positions. In this, only one team (Team 5 again) is found to be more than 3 places removed from the model prediction.\n",
    "\n",
    "Despite some limited success, it is clear we have some major pitfalls to address. Hence, the question: why did the model fall down in some cases, and by extension, how can we improve the model?\n",
    "\n",
    "We can start by considering a breakdown of the individual results, as in the histogram above. Considering each runner, we see the model on average is +11 seconds 'too optimistic' in predicting a mile time. From examining the shape of the histogram, we see exactly 1/3 of runners fall within +-11 seconds of their predicted time. The shape of this histogram loosely conforms to a Gaussian distribution, with tails of the distribution reaching > +-50 seconds from their predicted time. It is likely that these few data points are responsible for the inaccuracy in the final predictions. This offers a simple, but effective, explanation for Team 5's surprise victory. In this specific case, there was a single runner with a recorded 5k time of > 24 mins (in 2020), but was in fact capable of a < 20 mins 5k (as evidenced by past results). The model therefore assigned a prediction on this runner that was not only inaccurate, but one that could not be mitigated for by our Gaussian errors. This is an example of where our input data may be be flawed, and if inputting poor quality data, we would expect poor quality results!\n",
    "\n",
    "The nature of the input data is therefore of central importance. The main limitation is likely the selection of only a single, recent (2020) 5k time, as the standard to forecast from. An improved model would use a deeper set of input data, using not a single, but a range of (weighted) 5k times over longer timescales, i.e. pre-2020. A similar parameter which could be used to construct the model is a ranking/handicap score (such as those employed by RunBritain, see https://www.runbritainrankings.com/), which calculates a score for each runner based on past races, weighted by recency. This would help negate such disastrous miscalculations!   \n",
    "\n",
    "It's worth bearing in mind that this competition was run in unprecidented times, following three months without any races or training, after a national lockdown. The model used historic data of an event run without these restrictions, and with the aforementioned observation bias towards faster runners (who were more likely to both race mile distances, and do so consistently). The impact of the 2020 lockdown on amateur running participation and form is a topic well beyond the scope of this mini-review, but I believe it is fair to assume the loss of group training has reduced the speed endurance of the average club runner, which would cause the model to overestimate the pace of the entire sample. \n",
    "\n",
    "It is also debatable whether Gaussian errors can well describe the random variations of any individual in a race situation -- which have many unseen variables affecting performance. It seems likely that many more negative factors exist to influence a race result (e.g. weather, health, injury, form, training) than positive ones, and as such, these model errors may not be best represented as symmetric, and should possibly show wider tails to capture the wider random variation of race day. A student-t distribution may achieve this, or perhaps a lognormal distribution. Beyond this simple MC model, a more advanced model could implement a Bayesian framework and use a lognormal or beta prior distribution to model individual performances, in order to better account for this uncertainty.\n",
    "\n",
    "In summary, this toy-box model is successful only to a very limited extent, being most prominently restricted by the nature of input data to the model. It does however highlight an interesting case of how to build a simple MC model and visualise the results of the model, while remembering that for any model, the quality of input data is of paramount importance! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
